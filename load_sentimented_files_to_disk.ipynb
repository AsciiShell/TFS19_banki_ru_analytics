{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "load_sentimented_files_to_disk.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "cRxMc4ueIayq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Модель будем брать из диппавлова, потому что для русского языка моделей очень мало (deeppavlov и dostoevsky), а единственная модель из dostoevsky показывает скор ниже, чем модели из павлова, при том, что обучались они на одном датасете"
      ]
    },
    {
      "metadata": {
        "id": "mBMjLlcWuZZc",
        "colab_type": "code",
        "outputId": "7ce23f05-aa8b-4a5b-b149-cd05b18500a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2607
        }
      },
      "cell_type": "code",
      "source": [
        "! pip install deeppavlov"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting deeppavlov\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ab/92/33166dcd4fd87b171d5d37a87e19fc936e97f0a7ddbe2e7c0cdae7ceabb6/deeppavlov-0.2.0-py3-none-any.whl (602kB)\n",
            "\u001b[K    100% |████████████████████████████████| 604kB 25.6MB/s \n",
            "\u001b[?25hCollecting Cython==0.28.5 (from deeppavlov)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/8e/32b280abb0947a96cdbb8329fb2014851a21fc1d099009f946ea8a8202c3/Cython-0.28.5-cp36-cp36m-manylinux1_x86_64.whl (3.4MB)\n",
            "\u001b[K    100% |████████████████████████████████| 3.4MB 12.4MB/s \n",
            "\u001b[?25hCollecting pandas==0.23.1 (from deeppavlov)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/57/eb/6ab533ea8e35e7dd159af6922ac1123d4565d89f3926ad9a6aa46530978f/pandas-0.23.1-cp36-cp36m-manylinux1_x86_64.whl (11.8MB)\n",
            "\u001b[K    100% |████████████████████████████████| 11.8MB 3.5MB/s \n",
            "\u001b[?25hCollecting numpy==1.14.5 (from deeppavlov)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/68/1e/116ad560de97694e2d0c1843a7a0075cc9f49e922454d32f49a80eb6f1f2/numpy-1.14.5-cp36-cp36m-manylinux1_x86_64.whl (12.2MB)\n",
            "\u001b[K    100% |████████████████████████████████| 12.2MB 5.2MB/s \n",
            "\u001b[?25hCollecting pytelegrambotapi==3.5.2 (from deeppavlov)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/5a/7aab147b253f19e5ef007316f39cf693a63d5cd7f654c3805c76f6bde979/pyTelegramBotAPI-3.5.2.tar.gz (51kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 26.4MB/s \n",
            "\u001b[?25hCollecting requests==2.19.1 (from deeppavlov)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/65/47/7e02164a2a3db50ed6d8a6ab1d6d60b69c4c3fdf57a284257925dfc12bda/requests-2.19.1-py2.py3-none-any.whl (91kB)\n",
            "\u001b[K    100% |████████████████████████████████| 92kB 30.5MB/s \n",
            "\u001b[?25hCollecting flasgger==0.9.1 (from deeppavlov)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/4c/8735be27bebb88b0acbdc9db1d522583db10821aec3d3fb6112df0f41701/flasgger-0.9.1-py2.py3-none-any.whl (4.1MB)\n",
            "\u001b[K    100% |████████████████████████████████| 4.1MB 7.0MB/s \n",
            "\u001b[?25hCollecting flask-cors==3.0.6 (from deeppavlov)\n",
            "  Downloading https://files.pythonhosted.org/packages/d1/db/f3495569d5c3e2bdb9fb8a66c54503364abb6f35a9da2227cf5c9c50dc42/Flask_Cors-3.0.6-py2.py3-none-any.whl\n",
            "Requirement already satisfied: flask==1.0.2 in /usr/local/lib/python3.6/dist-packages (from deeppavlov) (1.0.2)\n",
            "Collecting rusenttokenize==0.0.4 (from deeppavlov)\n",
            "  Downloading https://files.pythonhosted.org/packages/f8/4c/e2aeee9cdcc266303289ad8c4acfdcf401781646bcc311ee2bf18f84d663/rusenttokenize-0.0.4-py3-none-any.whl\n",
            "Collecting overrides==1.9 (from deeppavlov)\n",
            "  Downloading https://files.pythonhosted.org/packages/de/55/3100c6d14c1ed177492fcf8f07c4a7d2d6c996c0a7fc6a9a0a41308e7eec/overrides-1.9.tar.gz\n",
            "Requirement already satisfied: h5py==2.8.0 in /usr/local/lib/python3.6/dist-packages (from deeppavlov) (2.8.0)\n",
            "Collecting pymorphy2==0.8 (from deeppavlov)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/33/fff9675c68b5f6c63ec8c6e6ff57827dda28a1fa5b2c2d727dffff92dd47/pymorphy2-0.8-py2.py3-none-any.whl (46kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 24.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk==3.2.5 in /usr/local/lib/python3.6/dist-packages (from deeppavlov) (3.2.5)\n",
            "Collecting pyopenssl==18.0.0 (from deeppavlov)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/96/af/9d29e6bd40823061aea2e0574ccb2fcf72bfd6130ce53d32773ec375458c/pyOpenSSL-18.0.0-py2.py3-none-any.whl (53kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 23.9MB/s \n",
            "\u001b[?25hCollecting keras==2.2.0 (from deeppavlov)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/68/12/4cabc5c01451eb3b413d19ea151f36e33026fc0efb932bf51bcaf54acbf5/Keras-2.2.0-py2.py3-none-any.whl (300kB)\n",
            "\u001b[K    100% |████████████████████████████████| 307kB 31.5MB/s \n",
            "\u001b[?25hCollecting tqdm==4.23.4 (from deeppavlov)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/93/24/6ab1df969db228aed36a648a8959d1027099ce45fad67532b9673d533318/tqdm-4.23.4-py2.py3-none-any.whl (42kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 25.5MB/s \n",
            "\u001b[?25hCollecting pymorphy2-dicts-ru (from deeppavlov)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/9b/358faaff410f65a4ad159275e897b5956dcb20576c5b8e764b971c1634d7/pymorphy2_dicts_ru-2.4.404381.4453942-py2.py3-none-any.whl (8.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 8.0MB 5.6MB/s \n",
            "\u001b[?25hCollecting scipy==1.1.0 (from deeppavlov)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/0b/f163da98d3a01b3e0ef1cab8dd2123c34aee2bafbb1c5bffa354cc8a1730/scipy-1.1.0-cp36-cp36m-manylinux1_x86_64.whl (31.2MB)\n",
            "\u001b[K    100% |████████████████████████████████| 31.2MB 1.2MB/s \n",
            "\u001b[?25hCollecting scikit-learn==0.19.1 (from deeppavlov)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3d/2d/9fbc7baa5f44bc9e88ffb7ed32721b879bfa416573e85031e16f52569bc9/scikit_learn-0.19.1-cp36-cp36m-manylinux1_x86_64.whl (12.4MB)\n",
            "\u001b[K    100% |████████████████████████████████| 12.4MB 3.9MB/s \n",
            "\u001b[?25hCollecting fuzzywuzzy==0.16.0 (from deeppavlov)\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/36/be990a35c7e8ed9dc176c43b5699cd971cec0b6f9ef858843374171df4f2/fuzzywuzzy-0.16.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas==0.23.1->deeppavlov) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas==0.23.1->deeppavlov) (2.5.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from pytelegrambotapi==3.5.2->deeppavlov) (1.11.0)\n",
            "Requirement already satisfied: urllib3<1.24,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests==2.19.1->deeppavlov) (1.22)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests==2.19.1->deeppavlov) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests==2.19.1->deeppavlov) (2019.3.9)\n",
            "Requirement already satisfied: idna<2.8,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests==2.19.1->deeppavlov) (2.6)\n",
            "Requirement already satisfied: jsonschema>=2.5.1 in /usr/local/lib/python3.6/dist-packages (from flasgger==0.9.1->deeppavlov) (2.6.0)\n",
            "Requirement already satisfied: mistune in /usr/local/lib/python3.6/dist-packages (from flasgger==0.9.1->deeppavlov) (0.8.4)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from flasgger==0.9.1->deeppavlov) (3.13)\n",
            "Requirement already satisfied: Werkzeug>=0.14 in /usr/local/lib/python3.6/dist-packages (from flask==1.0.2->deeppavlov) (0.15.2)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask==1.0.2->deeppavlov) (1.1.0)\n",
            "Requirement already satisfied: Jinja2>=2.10 in /usr/local/lib/python3.6/dist-packages (from flask==1.0.2->deeppavlov) (2.10.1)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from flask==1.0.2->deeppavlov) (7.0)\n",
            "Collecting dawg-python>=0.7 (from pymorphy2==0.8->deeppavlov)\n",
            "  Downloading https://files.pythonhosted.org/packages/6a/84/ff1ce2071d4c650ec85745766c0047ccc3b5036f1d03559fd46bb38b5eeb/DAWG_Python-0.7.2-py2.py3-none-any.whl\n",
            "Collecting pymorphy2-dicts<3.0,>=2.4 (from pymorphy2==0.8->deeppavlov)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/51/2465fd4f72328ab50877b54777764d928da8cb15b74e2680fc1bd8cb3173/pymorphy2_dicts-2.4.393442.3710985-py2.py3-none-any.whl (7.1MB)\n",
            "\u001b[K    100% |████████████████████████████████| 7.1MB 3.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.6/dist-packages (from pymorphy2==0.8->deeppavlov) (0.6.2)\n",
            "Collecting cryptography>=2.2.1 (from pyopenssl==18.0.0->deeppavlov)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5b/12/b0409a94dad366d98a8eee2a77678c7a73aafd8c0e4b835abea634ea3896/cryptography-2.6.1-cp34-abi3-manylinux1_x86_64.whl (2.3MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.3MB 14.5MB/s \n",
            "\u001b[?25hCollecting keras-applications==1.0.2 (from keras==2.2.0->deeppavlov)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e2/60/c557075e586e968d7a9c314aa38c236b37cb3ee6b37e8d57152b1a5e0b47/Keras_Applications-1.0.2-py2.py3-none-any.whl (43kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 26.7MB/s \n",
            "\u001b[?25hCollecting keras-preprocessing==1.0.1 (from keras==2.2.0->deeppavlov)\n",
            "  Downloading https://files.pythonhosted.org/packages/f8/33/275506afe1d96b221f66f95adba94d1b73f6b6087cfb6132a5655b6fe338/Keras_Preprocessing-1.0.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10->flask==1.0.2->deeppavlov) (1.1.1)\n",
            "Collecting asn1crypto>=0.21.0 (from cryptography>=2.2.1->pyopenssl==18.0.0->deeppavlov)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/cd/35485615f45f30a510576f1a56d1e0a7ad7bd8ab5ed7cdc600ef7cd06222/asn1crypto-0.24.0-py2.py3-none-any.whl (101kB)\n",
            "\u001b[K    100% |████████████████████████████████| 102kB 37.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi!=1.11.3,>=1.8 in /usr/local/lib/python3.6/dist-packages (from cryptography>=2.2.1->pyopenssl==18.0.0->deeppavlov) (1.12.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.2.1->pyopenssl==18.0.0->deeppavlov) (2.19)\n",
            "Building wheels for collected packages: pytelegrambotapi, overrides\n",
            "  Building wheel for pytelegrambotapi (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/b0/a0/fa/c3539fd47aa9f834230d64039c4bc620463bc7afc39b0f3f35\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/8d/52/86/e5a83b1797e7d263b458d2334edd2704c78508b3eea9323718\n",
            "Successfully built pytelegrambotapi overrides\n",
            "\u001b[31myellowbrick 0.9.1 has requirement scikit-learn>=0.20, but you'll have scikit-learn 0.19.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mtensorflow 1.13.1 has requirement keras-applications>=1.0.6, but you'll have keras-applications 1.0.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mtensorflow 1.13.1 has requirement keras-preprocessing>=1.0.5, but you'll have keras-preprocessing 1.0.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mspacy 2.0.18 has requirement numpy>=1.15.0, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mimgaug 0.2.8 has requirement numpy>=1.15.0, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mimbalanced-learn 0.4.3 has requirement scikit-learn>=0.20, but you'll have scikit-learn 0.19.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mgoogle-colab 1.0.0 has requirement requests~=2.18.0, but you'll have requests 2.19.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mfastai 1.0.51 has requirement numpy>=1.15, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mdatascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31malbumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.8 which is incompatible.\u001b[0m\n",
            "Installing collected packages: Cython, numpy, pandas, requests, pytelegrambotapi, flasgger, flask-cors, rusenttokenize, overrides, dawg-python, pymorphy2-dicts, pymorphy2, asn1crypto, cryptography, pyopenssl, keras-applications, scipy, keras-preprocessing, keras, tqdm, pymorphy2-dicts-ru, scikit-learn, fuzzywuzzy, deeppavlov\n",
            "  Found existing installation: Cython 0.29.6\n",
            "    Uninstalling Cython-0.29.6:\n",
            "      Successfully uninstalled Cython-0.29.6\n",
            "  Found existing installation: numpy 1.16.2\n",
            "    Uninstalling numpy-1.16.2:\n",
            "      Successfully uninstalled numpy-1.16.2\n",
            "  Found existing installation: pandas 0.23.4\n",
            "    Uninstalling pandas-0.23.4:\n",
            "      Successfully uninstalled pandas-0.23.4\n",
            "  Found existing installation: requests 2.18.4\n",
            "    Uninstalling requests-2.18.4:\n",
            "      Successfully uninstalled requests-2.18.4\n",
            "  Found existing installation: Keras-Applications 1.0.7\n",
            "    Uninstalling Keras-Applications-1.0.7:\n",
            "      Successfully uninstalled Keras-Applications-1.0.7\n",
            "  Found existing installation: scipy 1.2.1\n",
            "    Uninstalling scipy-1.2.1:\n",
            "      Successfully uninstalled scipy-1.2.1\n",
            "  Found existing installation: Keras-Preprocessing 1.0.9\n",
            "    Uninstalling Keras-Preprocessing-1.0.9:\n",
            "      Successfully uninstalled Keras-Preprocessing-1.0.9\n",
            "  Found existing installation: Keras 2.2.4\n",
            "    Uninstalling Keras-2.2.4:\n",
            "      Successfully uninstalled Keras-2.2.4\n",
            "  Found existing installation: tqdm 4.28.1\n",
            "    Uninstalling tqdm-4.28.1:\n",
            "      Successfully uninstalled tqdm-4.28.1\n",
            "  Found existing installation: scikit-learn 0.20.3\n",
            "    Uninstalling scikit-learn-0.20.3:\n",
            "      Successfully uninstalled scikit-learn-0.20.3\n",
            "Successfully installed Cython-0.28.5 asn1crypto-0.24.0 cryptography-2.6.1 dawg-python-0.7.2 deeppavlov-0.2.0 flasgger-0.9.1 flask-cors-3.0.6 fuzzywuzzy-0.16.0 keras-2.2.0 keras-applications-1.0.2 keras-preprocessing-1.0.1 numpy-1.14.5 overrides-1.9 pandas-0.23.1 pymorphy2-0.8 pymorphy2-dicts-2.4.393442.3710985 pymorphy2-dicts-ru-2.4.404381.4453942 pyopenssl-18.0.0 pytelegrambotapi-3.5.2 requests-2.19.1 rusenttokenize-0.0.4 scikit-learn-0.19.1 scipy-1.1.0 tqdm-4.23.4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "pandas",
                  "requests"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "yEsX0x_DImeO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Нужно еще установить fastText (он используется для эмбедингов) именно таким способом, а не просто через pip"
      ]
    },
    {
      "metadata": {
        "id": "LSR-tSd2IlMB",
        "colab_type": "code",
        "outputId": "f3aecbba-44eb-4b93-b33d-f5df7d81c0c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "! git clone https://github.com/facebookresearch/fastText.git\n",
        "os.chdir('/content/fastText')\n",
        "! pip install ."
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'fastText'...\n",
            "remote: Enumerating objects: 3192, done.\u001b[K\n",
            "remote: Total 3192 (delta 0), reused 0 (delta 0), pack-reused 3192\u001b[K\n",
            "Receiving objects: 100% (3192/3192), 7.84 MiB | 27.03 MiB/s, done.\n",
            "Resolving deltas: 100% (2004/2004), done.\n",
            "Processing /content/fastText\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.6/dist-packages (from fasttext==0.8.22) (2.2.4)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from fasttext==0.8.22) (40.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fasttext==0.8.22) (1.14.5)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-x48o_73b/wheels/a1/9f/52/696ce6c5c46325e840c76614ee5051458c0df10306987e7443\n",
            "Successfully built fasttext\n",
            "Installing collected packages: fasttext\n",
            "Successfully installed fasttext-0.8.22\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "b3oh1OEXJRkC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Проверяем запару с fastText, чтобы он импортился как fastText, а не как fasttext"
      ]
    },
    {
      "metadata": {
        "id": "dmcPgFrTI9XP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import fastText"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UgnnnRtwJcmT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Теперь импортим павлова"
      ]
    },
    {
      "metadata": {
        "id": "1jJvZZxjurMT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import deeppavlov as dp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a9CvWmGVJw7X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Импортим модель"
      ]
    },
    {
      "metadata": {
        "id": "YE6NqwcNPtzt",
        "colab_type": "code",
        "outputId": "2238f252-c1e1-496c-93c1-b32c8710a15e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2682
        }
      },
      "cell_type": "code",
      "source": [
        "elmo = dp.build_model(dp.configs.classifiers.rusentiment_elmo, download=True)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-04-22 15:56:33.669 INFO in 'deeppavlov.core.data.utils'['utils'] at line 63: Downloading from http://files.deeppavlov.ai/deeppavlov_data/classifiers/rusentiment_v4.tar.gz to /root/.deeppavlov/models/rusentiment_v4.tar.gz\n",
            "100%|██████████| 15.0M/15.0M [00:03<00:00, 4.04MB/s]\n",
            "2019-04-22 15:56:37.378 INFO in 'deeppavlov.core.data.utils'['utils'] at line 201: Extracting /root/.deeppavlov/models/rusentiment_v4.tar.gz archive into /root/.deeppavlov/models/classifiers\n",
            "2019-04-22 15:56:37.518 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 103: [loading vocabulary from /root/.deeppavlov/models/classifiers/rusentiment_v4/classes.dict]\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package perluniprops to /root/nltk_data...\n",
            "[nltk_data]   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data] Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0422 15:56:40.052372 140538693883776 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n",
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0422 15:58:23.859473 140538693883776 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0422 15:58:27.700643 140538693883776 saver.py:1483] Saver not created because there are no variables in the graph to restore\n",
            "2019-04-22 15:58:28.533 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 273: [initializing `KerasClassificationModel` from saved]\n",
            "I0422 15:58:28.533573 140538693883776 keras_classification_model.py:273] [initializing `KerasClassificationModel` from saved]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3363: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0422 15:58:28.881786 140538693883776 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3363: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "2019-04-22 15:58:29.106 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 283: [loading weights from model.h5]\n",
            "I0422 15:58:29.106869 140538693883776 keras_classification_model.py:283] [loading weights from model.h5]\n",
            "2019-04-22 15:58:29.405 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 134: Model was successfully initialized!\n",
            "Model summary:\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, None, 1024)   0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, None, 256)    786688      input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, None, 256)    1310976     input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, None, 256)    1835264     input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, None, 256)    1024        conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, None, 256)    1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, None, 256)    1024        conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, None, 256)    0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, None, 256)    0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, None, 256)    0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_1 (GlobalM (None, 256)          0           activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_2 (GlobalM (None, 256)          0           activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_3 (GlobalM (None, 256)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 768)          0           global_max_pooling1d_1[0][0]     \n",
            "                                                                 global_max_pooling1d_2[0][0]     \n",
            "                                                                 global_max_pooling1d_3[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 768)          0           concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 100)          76900       dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 100)          400         dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 100)          0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 100)          0           activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 5)            505         dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 5)            20          dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 5)            0           batch_normalization_5[0][0]      \n",
            "==================================================================================================\n",
            "Total params: 4,013,825\n",
            "Trainable params: 4,012,079\n",
            "Non-trainable params: 1,746\n",
            "__________________________________________________________________________________________________\n",
            "I0422 15:58:29.405230 140538693883776 keras_classification_model.py:134] Model was successfully initialized!\n",
            "Model summary:\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, None, 1024)   0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, None, 256)    786688      input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, None, 256)    1310976     input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, None, 256)    1835264     input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, None, 256)    1024        conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, None, 256)    1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, None, 256)    1024        conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, None, 256)    0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, None, 256)    0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, None, 256)    0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_1 (GlobalM (None, 256)          0           activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_2 (GlobalM (None, 256)          0           activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_3 (GlobalM (None, 256)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 768)          0           global_max_pooling1d_1[0][0]     \n",
            "                                                                 global_max_pooling1d_2[0][0]     \n",
            "                                                                 global_max_pooling1d_3[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 768)          0           concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 100)          76900       dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 100)          400         dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 100)          0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 100)          0           activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 5)            505         dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 5)            20          dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 5)            0           batch_normalization_5[0][0]      \n",
            "==================================================================================================\n",
            "Total params: 4,013,825\n",
            "Trainable params: 4,012,079\n",
            "Non-trainable params: 1,746\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "OL4InQdnTwsk",
        "colab_type": "code",
        "outputId": "b8b7270e-fc09-44c3-9f5a-e4e8959c1dd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QiQ_sH5wHnFa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from IPython.display import clear_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "upXb3exLCAea",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Sentimentit(object):\n",
        "  def __init__(self, files):\n",
        "    assert type(files) == list\n",
        "    self.file_list = files\n",
        "    self.number_of_files = len(files)\n",
        "    self.current_file = None\n",
        "    self.current_legth = None\n",
        "    \n",
        "  def yield_frame(self):\n",
        "    for frame in self.file_list:\n",
        "      self.current_file = frame\n",
        "      yield pd.read_pickle(os.path.join(frame, 'sentences_replies.pkl'))\n",
        "      \n",
        "  def yield_line(self, frame):\n",
        "    indexes = frame.index\n",
        "    sentences = frame['lemmatized']\n",
        "    for index, sentence in zip(indexes, sentences):\n",
        "      yield index, sentence\n",
        "      \n",
        "       \n",
        "      \n",
        "  def write_frame_to_file(self):\n",
        "    try:\n",
        "      count_frames = 0\n",
        "      gen_frames = self.yield_frame()\n",
        "      while True:\n",
        "          frame = next(gen_frames)\n",
        "          self.current_length = len(frame)\n",
        "          try:\n",
        "            gen_lines = self.yield_line(frame)\n",
        "            count_lines = 0\n",
        "            with open(os.path.join('/content', self.current_file + '_sentimented.csv'), 'w') as file:\n",
        "              file.write('id,label\\n')\n",
        "              while True:\n",
        "                index, sentence = next(gen_lines)\n",
        "                count_lines+=1\n",
        "                file.write(str(index) + ',' + elmo([sentence])[-1][-1]+'\\n')\n",
        "                if count_lines%100==0:\n",
        "                  clear_output()\n",
        "                  print('loaded {n} lines out of {m}\\n loaded {k} files out of 50'.format(n=count_lines, m=self.current_length, k=count_frames))\n",
        "          except StopIteration:\n",
        "            print('finished file {n}'.format(n=self.current_file))\n",
        "            count_frames+=1\n",
        "    except StopIteration:\n",
        "      print('finished')\n",
        "            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mtWhPMo1K_8T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "files = []\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5wVQWJF2Pupv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "os.chdir('/content/gdrive/My Drive/TopicModelingFintech')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WLLFbSI2P4mg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for line in os.listdir():\n",
        "  file = os.path.join(line, 'sentences_replies.pkl')\n",
        "  if os.path.exists(file):\n",
        "    files.append(line)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sF_0gPHfSxfd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "one = Sentimentit(files)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lilHehs9S1jI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "one.write_frame_to_file()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "llTxTCJRXZ71",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "os.chdir('/content')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y-C-lNHlXxh5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "for line in $(find -maxdepth 1 -name '*.csv')\n",
        "do\n",
        "mv $line '/content/gdrive/My Drive/banki ru csv'\n",
        "done"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eI4Fjm_Ncruk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}