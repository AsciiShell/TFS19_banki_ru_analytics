{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rusentiment_elmo.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "cRxMc4ueIayq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Модель будем брать из диппавлова, потому что для русского языка моделей очень мало (deeppavlov и dostoevsky), а единственная модель из dostoevsky показывает скор ниже, чем модели из павлова, при том, что обучались они на одном датасете"
      ]
    },
    {
      "metadata": {
        "id": "mBMjLlcWuZZc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2515
        },
        "outputId": "d83a9cd8-c7c7-4ca6-d021-b822dd079d8e"
      },
      "cell_type": "code",
      "source": [
        "! pip install deeppavlov"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting deeppavlov\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ab/92/33166dcd4fd87b171d5d37a87e19fc936e97f0a7ddbe2e7c0cdae7ceabb6/deeppavlov-0.2.0-py3-none-any.whl (602kB)\n",
            "\u001b[K    100% |████████████████████████████████| 604kB 21.2MB/s \n",
            "\u001b[?25hCollecting Cython==0.28.5 (from deeppavlov)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/8e/32b280abb0947a96cdbb8329fb2014851a21fc1d099009f946ea8a8202c3/Cython-0.28.5-cp36-cp36m-manylinux1_x86_64.whl (3.4MB)\n",
            "\u001b[K    100% |████████████████████████████████| 3.4MB 11.6MB/s \n",
            "\u001b[?25hCollecting fuzzywuzzy==0.16.0 (from deeppavlov)\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/36/be990a35c7e8ed9dc176c43b5699cd971cec0b6f9ef858843374171df4f2/fuzzywuzzy-0.16.0-py2.py3-none-any.whl\n",
            "Collecting flasgger==0.9.1 (from deeppavlov)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/4c/8735be27bebb88b0acbdc9db1d522583db10821aec3d3fb6112df0f41701/flasgger-0.9.1-py2.py3-none-any.whl (4.1MB)\n",
            "\u001b[K    100% |████████████████████████████████| 4.1MB 8.6MB/s \n",
            "\u001b[?25hCollecting pymorphy2-dicts-ru (from deeppavlov)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/9b/358faaff410f65a4ad159275e897b5956dcb20576c5b8e764b971c1634d7/pymorphy2_dicts_ru-2.4.404381.4453942-py2.py3-none-any.whl (8.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 8.0MB 1.0MB/s \n",
            "\u001b[?25hCollecting pyopenssl==18.0.0 (from deeppavlov)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/96/af/9d29e6bd40823061aea2e0574ccb2fcf72bfd6130ce53d32773ec375458c/pyOpenSSL-18.0.0-py2.py3-none-any.whl (53kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 20.2MB/s \n",
            "\u001b[?25hCollecting pandas==0.23.1 (from deeppavlov)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/57/eb/6ab533ea8e35e7dd159af6922ac1123d4565d89f3926ad9a6aa46530978f/pandas-0.23.1-cp36-cp36m-manylinux1_x86_64.whl (11.8MB)\n",
            "\u001b[K    100% |████████████████████████████████| 11.8MB 2.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk==3.2.5 in /usr/local/lib/python3.6/dist-packages (from deeppavlov) (3.2.5)\n",
            "Requirement already satisfied: h5py==2.8.0 in /usr/local/lib/python3.6/dist-packages (from deeppavlov) (2.8.0)\n",
            "Collecting pymorphy2==0.8 (from deeppavlov)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/33/fff9675c68b5f6c63ec8c6e6ff57827dda28a1fa5b2c2d727dffff92dd47/pymorphy2-0.8-py2.py3-none-any.whl (46kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 22.0MB/s \n",
            "\u001b[?25hCollecting keras==2.2.0 (from deeppavlov)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/68/12/4cabc5c01451eb3b413d19ea151f36e33026fc0efb932bf51bcaf54acbf5/Keras-2.2.0-py2.py3-none-any.whl (300kB)\n",
            "\u001b[K    100% |████████████████████████████████| 307kB 23.2MB/s \n",
            "\u001b[?25hCollecting pytelegrambotapi==3.5.2 (from deeppavlov)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/5a/7aab147b253f19e5ef007316f39cf693a63d5cd7f654c3805c76f6bde979/pyTelegramBotAPI-3.5.2.tar.gz (51kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 23.3MB/s \n",
            "\u001b[?25hCollecting scikit-learn==0.19.1 (from deeppavlov)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3d/2d/9fbc7baa5f44bc9e88ffb7ed32721b879bfa416573e85031e16f52569bc9/scikit_learn-0.19.1-cp36-cp36m-manylinux1_x86_64.whl (12.4MB)\n",
            "\u001b[K    100% |████████████████████████████████| 12.4MB 4.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: flask==1.0.2 in /usr/local/lib/python3.6/dist-packages (from deeppavlov) (1.0.2)\n",
            "Collecting requests==2.19.1 (from deeppavlov)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/65/47/7e02164a2a3db50ed6d8a6ab1d6d60b69c4c3fdf57a284257925dfc12bda/requests-2.19.1-py2.py3-none-any.whl (91kB)\n",
            "\u001b[K    100% |████████████████████████████████| 92kB 22.5MB/s \n",
            "\u001b[?25hCollecting rusenttokenize==0.0.4 (from deeppavlov)\n",
            "  Downloading https://files.pythonhosted.org/packages/f8/4c/e2aeee9cdcc266303289ad8c4acfdcf401781646bcc311ee2bf18f84d663/rusenttokenize-0.0.4-py3-none-any.whl\n",
            "Collecting flask-cors==3.0.6 (from deeppavlov)\n",
            "  Downloading https://files.pythonhosted.org/packages/d1/db/f3495569d5c3e2bdb9fb8a66c54503364abb6f35a9da2227cf5c9c50dc42/Flask_Cors-3.0.6-py2.py3-none-any.whl\n",
            "Collecting overrides==1.9 (from deeppavlov)\n",
            "  Downloading https://files.pythonhosted.org/packages/de/55/3100c6d14c1ed177492fcf8f07c4a7d2d6c996c0a7fc6a9a0a41308e7eec/overrides-1.9.tar.gz\n",
            "Collecting tqdm==4.23.4 (from deeppavlov)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/93/24/6ab1df969db228aed36a648a8959d1027099ce45fad67532b9673d533318/tqdm-4.23.4-py2.py3-none-any.whl (42kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 19.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy==1.1.0 in /usr/local/lib/python3.6/dist-packages (from deeppavlov) (1.1.0)\n",
            "Collecting numpy==1.14.5 (from deeppavlov)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/68/1e/116ad560de97694e2d0c1843a7a0075cc9f49e922454d32f49a80eb6f1f2/numpy-1.14.5-cp36-cp36m-manylinux1_x86_64.whl (12.2MB)\n",
            "\u001b[K    100% |████████████████████████████████| 12.2MB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: mistune in /usr/local/lib/python3.6/dist-packages (from flasgger==0.9.1->deeppavlov) (0.8.4)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from flasgger==0.9.1->deeppavlov) (1.11.0)\n",
            "Requirement already satisfied: jsonschema>=2.5.1 in /usr/local/lib/python3.6/dist-packages (from flasgger==0.9.1->deeppavlov) (2.6.0)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from flasgger==0.9.1->deeppavlov) (3.13)\n",
            "Collecting cryptography>=2.2.1 (from pyopenssl==18.0.0->deeppavlov)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5b/12/b0409a94dad366d98a8eee2a77678c7a73aafd8c0e4b835abea634ea3896/cryptography-2.6.1-cp34-abi3-manylinux1_x86_64.whl (2.3MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.3MB 9.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas==0.23.1->deeppavlov) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas==0.23.1->deeppavlov) (2.5.3)\n",
            "Collecting dawg-python>=0.7 (from pymorphy2==0.8->deeppavlov)\n",
            "  Downloading https://files.pythonhosted.org/packages/6a/84/ff1ce2071d4c650ec85745766c0047ccc3b5036f1d03559fd46bb38b5eeb/DAWG_Python-0.7.2-py2.py3-none-any.whl\n",
            "Collecting pymorphy2-dicts<3.0,>=2.4 (from pymorphy2==0.8->deeppavlov)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/51/2465fd4f72328ab50877b54777764d928da8cb15b74e2680fc1bd8cb3173/pymorphy2_dicts-2.4.393442.3710985-py2.py3-none-any.whl (7.1MB)\n",
            "\u001b[K    100% |████████████████████████████████| 7.1MB 4.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.6/dist-packages (from pymorphy2==0.8->deeppavlov) (0.6.2)\n",
            "Collecting keras-preprocessing==1.0.1 (from keras==2.2.0->deeppavlov)\n",
            "  Downloading https://files.pythonhosted.org/packages/f8/33/275506afe1d96b221f66f95adba94d1b73f6b6087cfb6132a5655b6fe338/Keras_Preprocessing-1.0.1-py2.py3-none-any.whl\n",
            "Collecting keras-applications==1.0.2 (from keras==2.2.0->deeppavlov)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e2/60/c557075e586e968d7a9c314aa38c236b37cb3ee6b37e8d57152b1a5e0b47/Keras_Applications-1.0.2-py2.py3-none-any.whl (43kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 20.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask==1.0.2->deeppavlov) (1.1.0)\n",
            "Requirement already satisfied: Jinja2>=2.10 in /usr/local/lib/python3.6/dist-packages (from flask==1.0.2->deeppavlov) (2.10)\n",
            "Requirement already satisfied: Werkzeug>=0.14 in /usr/local/lib/python3.6/dist-packages (from flask==1.0.2->deeppavlov) (0.15.2)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from flask==1.0.2->deeppavlov) (7.0)\n",
            "Requirement already satisfied: idna<2.8,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests==2.19.1->deeppavlov) (2.6)\n",
            "Requirement already satisfied: urllib3<1.24,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests==2.19.1->deeppavlov) (1.22)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests==2.19.1->deeppavlov) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests==2.19.1->deeppavlov) (2019.3.9)\n",
            "Collecting asn1crypto>=0.21.0 (from cryptography>=2.2.1->pyopenssl==18.0.0->deeppavlov)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/cd/35485615f45f30a510576f1a56d1e0a7ad7bd8ab5ed7cdc600ef7cd06222/asn1crypto-0.24.0-py2.py3-none-any.whl (101kB)\n",
            "\u001b[K    100% |████████████████████████████████| 102kB 27.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi!=1.11.3,>=1.8 in /usr/local/lib/python3.6/dist-packages (from cryptography>=2.2.1->pyopenssl==18.0.0->deeppavlov) (1.12.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10->flask==1.0.2->deeppavlov) (1.1.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.2.1->pyopenssl==18.0.0->deeppavlov) (2.19)\n",
            "Building wheels for collected packages: pytelegrambotapi, overrides\n",
            "  Building wheel for pytelegrambotapi (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/b0/a0/fa/c3539fd47aa9f834230d64039c4bc620463bc7afc39b0f3f35\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/8d/52/86/e5a83b1797e7d263b458d2334edd2704c78508b3eea9323718\n",
            "Successfully built pytelegrambotapi overrides\n",
            "\u001b[31myellowbrick 0.9.1 has requirement scikit-learn>=0.20, but you'll have scikit-learn 0.19.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mtensorflow 1.13.1 has requirement keras-applications>=1.0.6, but you'll have keras-applications 1.0.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mtensorflow 1.13.1 has requirement keras-preprocessing>=1.0.5, but you'll have keras-preprocessing 1.0.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mspacy 2.0.18 has requirement numpy>=1.15.0, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mimgaug 0.2.8 has requirement numpy>=1.15.0, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mimbalanced-learn 0.4.3 has requirement scikit-learn>=0.20, but you'll have scikit-learn 0.19.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mgoogle-colab 1.0.0 has requirement requests~=2.18.0, but you'll have requests 2.19.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mfastai 1.0.51 has requirement numpy>=1.15, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mdatascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31malbumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.8 which is incompatible.\u001b[0m\n",
            "Installing collected packages: Cython, fuzzywuzzy, flasgger, pymorphy2-dicts-ru, asn1crypto, cryptography, pyopenssl, numpy, pandas, dawg-python, pymorphy2-dicts, pymorphy2, keras-preprocessing, keras-applications, keras, requests, pytelegrambotapi, scikit-learn, rusenttokenize, flask-cors, overrides, tqdm, deeppavlov\n",
            "  Found existing installation: Cython 0.29.6\n",
            "    Uninstalling Cython-0.29.6:\n",
            "      Successfully uninstalled Cython-0.29.6\n",
            "  Found existing installation: numpy 1.14.6\n",
            "    Uninstalling numpy-1.14.6:\n",
            "      Successfully uninstalled numpy-1.14.6\n",
            "  Found existing installation: pandas 0.22.0\n",
            "    Uninstalling pandas-0.22.0:\n",
            "      Successfully uninstalled pandas-0.22.0\n",
            "  Found existing installation: Keras-Preprocessing 1.0.9\n",
            "    Uninstalling Keras-Preprocessing-1.0.9:\n",
            "      Successfully uninstalled Keras-Preprocessing-1.0.9\n",
            "  Found existing installation: Keras-Applications 1.0.7\n",
            "    Uninstalling Keras-Applications-1.0.7:\n",
            "      Successfully uninstalled Keras-Applications-1.0.7\n",
            "  Found existing installation: Keras 2.2.4\n",
            "    Uninstalling Keras-2.2.4:\n",
            "      Successfully uninstalled Keras-2.2.4\n",
            "  Found existing installation: requests 2.18.4\n",
            "    Uninstalling requests-2.18.4:\n",
            "      Successfully uninstalled requests-2.18.4\n",
            "  Found existing installation: scikit-learn 0.20.3\n",
            "    Uninstalling scikit-learn-0.20.3:\n",
            "      Successfully uninstalled scikit-learn-0.20.3\n",
            "  Found existing installation: tqdm 4.28.1\n",
            "    Uninstalling tqdm-4.28.1:\n",
            "      Successfully uninstalled tqdm-4.28.1\n",
            "Successfully installed Cython-0.28.5 asn1crypto-0.24.0 cryptography-2.6.1 dawg-python-0.7.2 deeppavlov-0.2.0 flasgger-0.9.1 flask-cors-3.0.6 fuzzywuzzy-0.16.0 keras-2.2.0 keras-applications-1.0.2 keras-preprocessing-1.0.1 numpy-1.14.5 overrides-1.9 pandas-0.23.1 pymorphy2-0.8 pymorphy2-dicts-2.4.393442.3710985 pymorphy2-dicts-ru-2.4.404381.4453942 pyopenssl-18.0.0 pytelegrambotapi-3.5.2 requests-2.19.1 rusenttokenize-0.0.4 scikit-learn-0.19.1 tqdm-4.23.4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "pandas",
                  "requests"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "yEsX0x_DImeO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Нужно еще установить fastText (он используется для эмбедингов) именно таким способом, а не просто через pip"
      ]
    },
    {
      "metadata": {
        "id": "LSR-tSd2IlMB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "outputId": "ac57ed0b-8222-488a-8ecc-fe68c1b2500f"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "! git clone https://github.com/facebookresearch/fastText.git\n",
        "os.chdir('/content/fastText')\n",
        "! pip install ."
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'fastText'...\n",
            "remote: Enumerating objects: 16, done.\u001b[K\n",
            "remote: Counting objects: 100% (16/16), done.\u001b[K\n",
            "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
            "remote: Total 3137 (delta 2), reused 13 (delta 2), pack-reused 3121\u001b[K\n",
            "Receiving objects: 100% (3137/3137), 7.82 MiB | 9.91 MiB/s, done.\n",
            "Resolving deltas: 100% (1968/1968), done.\n",
            "Processing /content/fastText\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.6/dist-packages (from fasttext==0.8.22) (2.2.4)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from fasttext==0.8.22) (40.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fasttext==0.8.22) (1.14.5)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-wpmb7xo4/wheels/a1/9f/52/696ce6c5c46325e840c76614ee5051458c0df10306987e7443\n",
            "Successfully built fasttext\n",
            "Installing collected packages: fasttext\n",
            "Successfully installed fasttext-0.8.22\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "b3oh1OEXJRkC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Проверяем запару с fastText, чтобы он импортился как fastText, а не как fasttext"
      ]
    },
    {
      "metadata": {
        "id": "dmcPgFrTI9XP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import fastText"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UgnnnRtwJcmT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Теперь импортим павлова"
      ]
    },
    {
      "metadata": {
        "id": "1jJvZZxjurMT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import deeppavlov as dp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a9CvWmGVJw7X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Импортим модель"
      ]
    },
    {
      "metadata": {
        "id": "n4a9YdLTvCD0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1561
        },
        "outputId": "0b6dc90a-348e-459e-8412-fe613daea6f7"
      },
      "cell_type": "code",
      "source": [
        "model = dp.build_model(dp.configs.classifiers.sentiment_twitter, download=True)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-04-08 22:52:41.532 INFO in 'deeppavlov.core.data.utils'['utils'] at line 63: Downloading from http://files.deeppavlov.ai/datasets/sentiment_twitter_data.tar.gz to /root/.deeppavlov/sentiment_twitter_data.tar.gz\n",
            "100%|██████████| 11.9M/11.9M [00:00<00:00, 12.2MB/s]\n",
            "2019-04-08 22:52:42.513 INFO in 'deeppavlov.core.data.utils'['utils'] at line 201: Extracting /root/.deeppavlov/sentiment_twitter_data.tar.gz archive into /root/.deeppavlov/downloads\n",
            "2019-04-08 22:52:43.14 INFO in 'deeppavlov.core.data.utils'['utils'] at line 63: Downloading from http://files.deeppavlov.ai/embeddings/ft_native_300_ru_wiki_lenta_nltk_wordpunct_tokenize/ft_native_300_ru_wiki_lenta_nltk_wordpunct_tokenize.bin to /root/.deeppavlov/downloads/embeddings/ft_native_300_ru_wiki_lenta_nltk_wordpunct_tokenize.bin\n",
            "100%|██████████| 6.19G/6.19G [18:01<00:00, 5.72MB/s]\n",
            "2019-04-08 23:10:44.313 INFO in 'deeppavlov.core.data.utils'['utils'] at line 63: Downloading from http://files.deeppavlov.ai/deeppavlov_data/classifiers/sentiment_twitter_v6.tar.gz to /root/.deeppavlov/models/sentiment_twitter_v6.tar.gz\n",
            "100%|██████████| 4.61M/4.61M [00:00<00:00, 5.18MB/s]\n",
            "2019-04-08 23:10:45.207 INFO in 'deeppavlov.core.data.utils'['utils'] at line 201: Extracting /root/.deeppavlov/models/sentiment_twitter_v6.tar.gz archive into /root/.deeppavlov/models/classifiers\n",
            "2019-04-08 23:10:45.266 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 103: [loading vocabulary from /root/.deeppavlov/models/classifiers/sentiment_twitter_v6/classes.dict]\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package perluniprops to /root/nltk_data...\n",
            "[nltk_data]   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data] Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "2019-04-08 23:10:47.37 INFO in 'deeppavlov.models.embedders.fasttext_embedder'['fasttext_embedder'] at line 52: [loading fastText embeddings from `/root/.deeppavlov/downloads/embeddings/ft_native_300_ru_wiki_lenta_nltk_wordpunct_tokenize.bin`]\n",
            "Using TensorFlow backend.\n",
            "2019-04-08 23:11:35.583 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 273: [initializing `KerasClassificationModel` from saved]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3363: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-04-08 23:11:36.484 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 283: [loading weights from model.h5]\n",
            "2019-04-08 23:11:36.990 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 134: Model was successfully initialized!\n",
            "Model summary:\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, None, 300)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, None, 256)    230656      input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, None, 256)    384256      input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, None, 256)    537856      input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, None, 256)    1024        conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, None, 256)    1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, None, 256)    1024        conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, None, 256)    0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, None, 256)    0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, None, 256)    0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_1 (GlobalM (None, 256)          0           activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_2 (GlobalM (None, 256)          0           activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_3 (GlobalM (None, 256)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 768)          0           global_max_pooling1d_1[0][0]     \n",
            "                                                                 global_max_pooling1d_2[0][0]     \n",
            "                                                                 global_max_pooling1d_3[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 768)          0           concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 100)          76900       dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 100)          400         dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 100)          0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 100)          0           activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 2)            202         dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 2)            8           dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 2)            0           batch_normalization_5[0][0]      \n",
            "==================================================================================================\n",
            "Total params: 1,233,350\n",
            "Trainable params: 1,231,610\n",
            "Non-trainable params: 1,740\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "SJE-1fMuPJRK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Проверяем"
      ]
    },
    {
      "metadata": {
        "id": "-1OzPEZzOr6-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3bf7219f-7051-41da-c6a4-135f1851efb2"
      },
      "cell_type": "code",
      "source": [
        "model(['Все хорошо', 'Все плохо'])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Negative'], ['Negative']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "fOb0-DEHPQzr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Ну, такое"
      ]
    },
    {
      "metadata": {
        "id": "BWNs926wPg9W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Попробуем другую модель"
      ]
    },
    {
      "metadata": {
        "id": "YE6NqwcNPtzt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2682
        },
        "outputId": "1c372606-df02-44ee-b55e-e62d48008616"
      },
      "cell_type": "code",
      "source": [
        "elmo = dp.build_model(dp.configs.classifiers.rusentiment_elmo, download=True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-04-08 23:34:22.211 INFO in 'deeppavlov.core.data.utils'['utils'] at line 63: Downloading from http://files.deeppavlov.ai/deeppavlov_data/classifiers/rusentiment_v4.tar.gz to /root/.deeppavlov/models/rusentiment_v4.tar.gz\n",
            "100%|██████████| 15.0M/15.0M [00:02<00:00, 5.04MB/s]\n",
            "2019-04-08 23:34:25.189 INFO in 'deeppavlov.core.data.utils'['utils'] at line 201: Extracting /root/.deeppavlov/models/rusentiment_v4.tar.gz archive into /root/.deeppavlov/models/classifiers\n",
            "2019-04-08 23:34:25.366 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 103: [loading vocabulary from /root/.deeppavlov/models/classifiers/rusentiment_v4/classes.dict]\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package perluniprops to /root/nltk_data...\n",
            "[nltk_data]   Package perluniprops is already up-to-date!\n",
            "[nltk_data] Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package nonbreaking_prefixes is already up-to-date!\n",
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0408 23:34:27.754464 140718139815808 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n",
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0408 23:34:30.549331 140718139815808 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0408 23:34:33.037093 140718139815808 saver.py:1483] Saver not created because there are no variables in the graph to restore\n",
            "2019-04-08 23:34:33.941 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 273: [initializing `KerasClassificationModel` from saved]\n",
            "I0408 23:34:33.941193 140718139815808 keras_classification_model.py:273] [initializing `KerasClassificationModel` from saved]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3363: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0408 23:34:34.373614 140718139815808 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3363: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "2019-04-08 23:34:34.668 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 283: [loading weights from model.h5]\n",
            "I0408 23:34:34.668511 140718139815808 keras_classification_model.py:283] [loading weights from model.h5]\n",
            "2019-04-08 23:34:35.0 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 134: Model was successfully initialized!\n",
            "Model summary:\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, None, 1024)   0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, None, 256)    786688      input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, None, 256)    1310976     input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, None, 256)    1835264     input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, None, 256)    1024        conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, None, 256)    1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, None, 256)    1024        conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, None, 256)    0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, None, 256)    0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, None, 256)    0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_1 (GlobalM (None, 256)          0           activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_2 (GlobalM (None, 256)          0           activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_3 (GlobalM (None, 256)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 768)          0           global_max_pooling1d_1[0][0]     \n",
            "                                                                 global_max_pooling1d_2[0][0]     \n",
            "                                                                 global_max_pooling1d_3[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 768)          0           concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 100)          76900       dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 100)          400         dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 100)          0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 100)          0           activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 5)            505         dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 5)            20          dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 5)            0           batch_normalization_5[0][0]      \n",
            "==================================================================================================\n",
            "Total params: 4,013,825\n",
            "Trainable params: 4,012,079\n",
            "Non-trainable params: 1,746\n",
            "__________________________________________________________________________________________________\n",
            "I0408 23:34:35.000782 140718139815808 keras_classification_model.py:134] Model was successfully initialized!\n",
            "Model summary:\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, None, 1024)   0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, None, 256)    786688      input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, None, 256)    1310976     input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, None, 256)    1835264     input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, None, 256)    1024        conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, None, 256)    1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, None, 256)    1024        conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, None, 256)    0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, None, 256)    0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, None, 256)    0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_1 (GlobalM (None, 256)          0           activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_2 (GlobalM (None, 256)          0           activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_3 (GlobalM (None, 256)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 768)          0           global_max_pooling1d_1[0][0]     \n",
            "                                                                 global_max_pooling1d_2[0][0]     \n",
            "                                                                 global_max_pooling1d_3[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 768)          0           concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 100)          76900       dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 100)          400         dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 100)          0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 100)          0           activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 5)            505         dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 5)            20          dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 5)            0           batch_normalization_5[0][0]      \n",
            "==================================================================================================\n",
            "Total params: 4,013,825\n",
            "Trainable params: 4,012,079\n",
            "Non-trainable params: 1,746\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "nSNCs31LTsN_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Проверяем"
      ]
    },
    {
      "metadata": {
        "id": "igSQO9wHSNF8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "f72ed127-2f6e-4674-d303-a3d2aefcaea3"
      },
      "cell_type": "code",
      "source": [
        "elmo(['все ок', 'все плохо', 'отвратительный фильмы', 'чудесное настроение!', 'как зодолбали эти неимпортящиеся библиотеки', 'как хорош павлов'])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['neutral'],\n",
              " ['negative'],\n",
              " ['negative'],\n",
              " ['positive'],\n",
              " ['negative'],\n",
              " ['positive']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "dC8i8PKmUbCO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Победитель определился"
      ]
    },
    {
      "metadata": {
        "id": "pVnHKmgRVJGA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Эта модель обучалась на RuSentiment: датасет русскоязычных постов в блогах и соцсетях (предыдущая модель обучалась на русском твитере), c elmo-эмбедингами, которые, насколько я понял, обучались на русском твитере. Был выбор между двумя типами моделей из павлова: те, которые обучались на русском твитере (аутпут: негативный или положительный) и те, которые обучались на постах в соц сетях (5 категорий тональности), скоры по ним сравнить нельзя было (в доке для моделей первого типа приводилась только accuracy, для моделей второго типа - f-мера), так как эти две метрики не коррелируют, поэтому попробовали оба класса моделей, причем модель первого класса (model) показывала лучший скор из моделей, обучавшихся на твитере (поэтому остальные пробовать вряд ли имеет смысл). Модель с elmo-эмбедингами тоже показывает лучший скор среди моделей, обучавшихся на постах, поэтому можно остановиться на ней, очень быстро скачивается, легко использовать, легко будет интегрироваться в код (всего-то пару импортов), в крайнем случае, можно будет попробовать другие, но это уже по результатам основного этапа"
      ]
    },
    {
      "metadata": {
        "id": "OL4InQdnTwsk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}