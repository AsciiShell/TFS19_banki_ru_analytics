{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "banki ru.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "AtXi27a9Jg2e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Чтобы запустить, просто можете поменять дирректории. Если не хочется в колабе это делать, то могу переделать, просто дайте знать"
      ]
    },
    {
      "metadata": {
        "id": "9q33l9-ZKWfd",
        "colab_type": "code",
        "outputId": "ce2f85f1-9ef9-4d2f-84f3-4cb06d129499",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WXAmI4RpKgXQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython.display import clear_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "17uWDwvNLBUU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "os.chdir('/content/gdrive/My Drive/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TkjedA47LGYF",
        "colab_type": "code",
        "outputId": "d92d904f-f75b-46f8-894b-f7f82d43ee26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "os.getcwd()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/gdrive/My Drive'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "ftaWuoOMGyWc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Разархивирование основного архива"
      ]
    },
    {
      "metadata": {
        "id": "G3yshg0ELIjl",
        "colab_type": "code",
        "outputId": "31dfc403-ca16-4f09-8d92-5d2ec0540e52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 954
        }
      },
      "cell_type": "code",
      "source": [
        "! tar -xzvf bankiru.tar.gz"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TopicModelingFintech/\n",
            "TopicModelingFintech/vozrozhdenie.tar.gz\n",
            "TopicModelingFintech/sberbank.tar.gz\n",
            "TopicModelingFintech/gazprombank.tar.gz\n",
            "TopicModelingFintech/mkb.tar.gz\n",
            "TopicModelingFintech/raiffeisen.tar.gz\n",
            "TopicModelingFintech/rgsbank.tar.gz\n",
            "TopicModelingFintech/locko-bank.tar.gz\n",
            "TopicModelingFintech/touchbank.tar.gz\n",
            "TopicModelingFintech/bcs-bank.tar.gz\n",
            "TopicModelingFintech/rsb.tar.gz\n",
            "TopicModelingFintech/modulbank.tar.gz\n",
            "TopicModelingFintech/fk_otkritie.tar.gz\n",
            "TopicModelingFintech/minbank.tar.gz\n",
            "TopicModelingFintech/citibank.tar.gz\n",
            "TopicModelingFintech/trust.tar.gz\n",
            "TopicModelingFintech/avangard.tar.gz\n",
            "TopicModelingFintech/jugra.tar.gz\n",
            "TopicModelingFintech/binbankcreditcard.tar.gz\n",
            "TopicModelingFintech/roscap.tar.gz\n",
            "TopicModelingFintech/absolutbank.tar.gz\n",
            "TopicModelingFintech/transcapitalbank.tar.gz\n",
            "TopicModelingFintech/yandexdengi.tar.gz\n",
            "TopicModelingFintech/crediteuropebank.tar.gz\n",
            "TopicModelingFintech/ubrr.tar.gz\n",
            "TopicModelingFintech/homecreditbank.tar.gz\n",
            "TopicModelingFintech/rencredit.tar.gz\n",
            "TopicModelingFintech/vtb.tar.gz\n",
            "TopicModelingFintech/v-express-bank.tar.gz\n",
            "TopicModelingFintech/mosoblbank.tar.gz\n",
            "TopicModelingFintech/alfabank.tar.gz\n",
            "TopicModelingFintech/deltacredit.tar.gz\n",
            "TopicModelingFintech/qiwibank.tar.gz\n",
            "TopicModelingFintech/sovcombank.tar.gz\n",
            "TopicModelingFintech/rshb.tar.gz\n",
            "TopicModelingFintech/bnpparibaseast.tar.gz\n",
            "TopicModelingFintech/akbars.tar.gz\n",
            "TopicModelingFintech/promsvyazbank.tar.gz\n",
            "TopicModelingFintech/otpbank.tar.gz\n",
            "TopicModelingFintech/uralsib.tar.gz\n",
            "TopicModelingFintech/pochtabank.tar.gz\n",
            "TopicModelingFintech/smpbank.tar.gz\n",
            "TopicModelingFintech/rusfinancebank.tar.gz\n",
            "TopicModelingFintech/rosbank.tar.gz\n",
            "TopicModelingFintech/mts-bank.tar.gz\n",
            "TopicModelingFintech/tcs.tar.gz\n",
            "TopicModelingFintech/binbank.tar.gz\n",
            "TopicModelingFintech/skb-bank.tar.gz\n",
            "TopicModelingFintech/vtb24.tar.gz\n",
            "TopicModelingFintech/bspb.tar.gz\n",
            "TopicModelingFintech/unicreditbank.tar.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "b9ouLh6gODgB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "os.chdir('/content/gdrive/My Drive/TopicModelingFintech')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "koXdBPiZOQw9",
        "colab_type": "code",
        "outputId": "c9f331f1-b7ea-4869-eb7e-dc777816aa7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "os.getcwd()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/gdrive/My Drive/TopicModelingFintech'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "id": "WNotF1YFG4Ak",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Разархивирование подархивов"
      ]
    },
    {
      "metadata": {
        "id": "sFNxJzUeLOSy",
        "colab_type": "code",
        "outputId": "0656111e-2b9d-4f63-8bea-9dbfffa03e94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        }
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "for file in $(find )\n",
        "do\n",
        "tar -xzf $file\n",
        "done"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tar (child): .: Cannot read: Is a directory\n",
            "tar (child): At beginning of tape, quitting now\n",
            "tar (child): Error is not recoverable: exiting now\n",
            "\n",
            "gzip: stdin: unexpected end of file\n",
            "tar: Child returned status 2\n",
            "tar: Error is not recoverable: exiting now\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "ES1Z0ghaG-XZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Создание файла, в котором будем хранить названия файлов с текстами (pickls с текстами)"
      ]
    },
    {
      "metadata": {
        "id": "4Pd4KU9uRQ4H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "! touch '/content/files.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qx4grmEEAwTn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open('/content/files.txt', 'w') as file:\n",
        "  for line in os.listdir('/content/gdrive/My Drive/TopicModelingFintech'):\n",
        "    file.write(os.path.join(os.getcwd(), line, 'sentences_replies.pkl') + '\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VwYTIH2rHZA3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Скачиваем эмбединги из диппавлова"
      ]
    },
    {
      "metadata": {
        "id": "hGDQBiIinuCK",
        "colab_type": "code",
        "outputId": "6ba39015-c4be-4ad3-a742-893ec9c6e9fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "cell_type": "code",
      "source": [
        "! wget 'files.deeppavlov.ai/embeddings/ft_native_300_ru_twitter_nltk_word_tokenize.vec'"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-04-14 23:49:46--  http://files.deeppavlov.ai/embeddings/ft_native_300_ru_twitter_nltk_word_tokenize.vec\n",
            "Resolving files.deeppavlov.ai (files.deeppavlov.ai)... 93.175.29.74\n",
            "Connecting to files.deeppavlov.ai (files.deeppavlov.ai)|93.175.29.74|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2767469831 (2.6G) [application/octet-stream]\n",
            "Saving to: ‘ft_native_300_ru_twitter_nltk_word_tokenize.vec’\n",
            "\n",
            "ft_native_300_ru_tw 100%[===================>]   2.58G  5.04MB/s    in 9m 49s  \n",
            "\n",
            "2019-04-14 23:59:36 (4.48 MB/s) - ‘ft_native_300_ru_twitter_nltk_word_tokenize.vec’ saved [2767469831/2767469831]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BOLJGIAxBvzJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "os.chdir('/content')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-HnklVRrHh8W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Дальше я делаю словарь слов из всего корпуса, чтобы загружать в ram только эмбединги для этих слов, так как больше у меня не вмещается ни в колабе, где вообще ограничение есть (12 гб), ни на локальном компухтере, который начинает очень тупить  после загрузки 1.5 млн эмбедингов, кто-нибудь помогите, как это по-нормальному делается"
      ]
    },
    {
      "metadata": {
        "id": "EK7zCwmn-24P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def gen_frame():\n",
        "    with open('files.txt') as file:\n",
        "        for line in file:\n",
        "            if os.path.exists(line.strip('\\n')):\n",
        "                yield pd.read_pickle(line.strip('\\n'))['lemmatized']\n",
        "gen = gen_frame()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ovhdLrDr_NiX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "fd4598fc-9a72-417e-8d87-39cb65c2da54"
      },
      "cell_type": "code",
      "source": [
        "vocab = {}\n",
        "count_frames = 0\n",
        "try:\n",
        "    while True:\n",
        "        frame = next(gen)\n",
        "        for line in frame:\n",
        "            for word in line.split():\n",
        "                if word not in vocab:\n",
        "                    vocab[word]=np.random.normal(size=(100,))\n",
        "        count_frames+=1\n",
        "        if count_frames%5==0:\n",
        "            clear_output()\n",
        "            print('loaded {n} frames out of 50'.format(n=count_frames))\n",
        "except StopIteration:\n",
        "    print('finished')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loaded 50 frames out of 50\n",
            "finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Q5PsBElwIF4M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Загружаю только нужные эмбединги"
      ]
    },
    {
      "metadata": {
        "id": "hcCCiWbHNxsg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "f14d41ff-6eb0-4fcc-8c44-85ccb0dd1d15"
      },
      "cell_type": "code",
      "source": [
        "def gen_line_from_file():\n",
        "  with open('/content/ft_native_300_ru_twitter_nltk_word_tokenize.vec') as file:\n",
        "    file.readline()\n",
        "    for line in file:\n",
        "      line = line.split()\n",
        "      yield line[0], list(map(float, line[1:]))\n",
        "\n",
        "g = gen_line_from_file()\n",
        "\n",
        "def make_vocab_from_file(gen, vocab):\n",
        "    count=0\n",
        "    try:\n",
        "        while True:\n",
        "            word, emb = next(gen)\n",
        "            if word in vocab:\n",
        "                vocab[word] = emb\n",
        "                count+=1\n",
        "                if count%1000==0:\n",
        "                    clear_output()\n",
        "                    print('Loaded {n} out of {m} embeddings'.format(n=count, m=len(vocab)))\n",
        "    except StopIteration:\n",
        "        print('finished')\n",
        "\n",
        "make_vocab_from_file(g, vocab)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded 52000 out of 97173 embeddings\n",
            "finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "W6xtd6tpIKay",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Получилось, что у нас есть только половина, поэтому нужно обучить свои эмбединги"
      ]
    },
    {
      "metadata": {
        "id": "Wp4mSaKHrvnm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from gensim.models import FastText"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sqF8osdeKl-n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size=30\n",
        "\n",
        "def streamer():\n",
        "  try:\n",
        "    while True:\n",
        "      frame = next(gen)\n",
        "      for i in range(len(frame.index)//batch_size):\n",
        "        inds = frame.index[i:i+batch_size]\n",
        "        yield frame.loc[inds]\n",
        "  except StopIteration:\n",
        "    print('finished training')\n",
        "\n",
        "stream = streamer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aRSF3DjDXnz7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Streamer(object):\n",
        "  def __init__(self, generator):\n",
        "    self.gen = generator\n",
        "    \n",
        "  def __iter__(self):\n",
        "    return self\n",
        "    \n",
        "  def __next__(self):\n",
        "    return next(self.gen)\n",
        "  \n",
        "itor = Streamer(stream)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y-OYZcu1WDKQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def hashit(argument=None):\n",
        "  matrix = np.empty(shape=(97173, 100))\n",
        "  for index, word in enumerate(vocab.keys()):\n",
        "    matrix[index] = vocab[word]\n",
        "  return matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8Udk9fFKKDIO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2b0eb241-529d-45ee-bf30-c7048aa962cf"
      },
      "cell_type": "code",
      "source": [
        "model = FastText(sentences=itor, size=100, workers=-1, hs=0, negative=10, ns_exponent=0.75)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "finished training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1h9M6pdRIWrt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Хотелось использовать эмбединги из диппавлова, как инициализацию фасттекста, но как-то не получилось, я просто не знаю, как, видимо, через функцию hashfnx, но принцип ее работы я как-то пока не понял"
      ]
    },
    {
      "metadata": {
        "id": "o9EexFryItvj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Теперь эмбедим предложения средним по всем эмбедингам слов и пускаем в дбскан. Параметры подбирались, чтобы классов было в районе 5, по-моему, это адекватно, может, нет. В дальнейшем это все будет лучше. Для примера данные по 5 банкам, то есть метки отзывов по ним по классам можно получить, запустив код. По определению тематик пока что нормального нет ничего, но скоро будет"
      ]
    },
    {
      "metadata": {
        "id": "UB-owEJZ3rpM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open('/content/gdrive/My Drive/TopicModelingFintech/model_config', 'wb') as file:\n",
        "  model.save(file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dxY7N--Xkv1J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "pattern = re.compile(r'(?<=/content/gdrive/My Drive/TopicModelingFintech/)[A-Za-z0-9-_]*(?=/sentences_replies.pkl)')\n",
        "\n",
        "def gen_frame_with_name():\n",
        "    with open('files.txt') as file:\n",
        "        for line in file:\n",
        "            if os.path.exists(line.strip('\\n')):\n",
        "                name = re.findall(pattern, line)\n",
        "                yield pd.read_pickle(line.strip('\\n'))['lemmatized'], name[0]\n",
        "                \n",
        "gen = gen_frame_with_name()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Th8YPUw8MHhp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def gen_embeded_corpus(frame):\n",
        "  corpus = []\n",
        "  \n",
        "  for line in frame:\n",
        "    line = line.split()\n",
        "    matrix = np.random.normal(size=(len(line), 100))\n",
        "    for index, word in enumerate(line):\n",
        "      if word in model.wv:\n",
        "        matrix[index] = model.wv[word]\n",
        "    corpus.append(np.mean(matrix, axis=0))\n",
        "  \n",
        "  return np.asarray(corpus)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oYdyzEUVo5CE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "5dbe3852-108b-430d-b89e-cb9a752131cf"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "clusterer = DBSCAN(eps=0.01, min_samples=5)\n",
        "\n",
        "count = 0\n",
        "\n",
        "try:\n",
        "  with open(os.path.join('/content', 'labels.csv'), 'w') as file:\n",
        "    file.write('title,labels\\n')\n",
        "    while True:\n",
        "      frame, name = next(gen)\n",
        "    \n",
        "      output = clusterer.fit_predict(X=gen_embeded_corpus(frame))\n",
        "    \n",
        "      file.write(name + ',' + ' '.join(list(map(str, output))) + '\\n')\n",
        "      count+=1\n",
        "      if count==5:\n",
        "        raise StopIteration\n",
        "      clear_output()\n",
        "      print('loaded {n} frames out of max 50'.format(n=count))\n",
        "except StopIteration:\n",
        "  print('finished')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loaded 4 frames out of max 50\n",
            "finished\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}